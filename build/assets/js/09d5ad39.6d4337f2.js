"use strict";(self.webpackChunkbuildflow_docs=self.webpackChunkbuildflow_docs||[]).push([[3030],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>f});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var u=a.createContext({}),s=function(e){var t=a.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=s(e.components);return a.createElement(u.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,u=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=s(n),d=o,f=c["".concat(u,".").concat(d)]||c[d]||m[d]||r;return n?a.createElement(f,i(i({ref:t},p),{},{components:n})):a.createElement(f,i({ref:t},p))}));function f(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=d;var l={};for(var u in t)hasOwnProperty.call(t,u)&&(l[u]=t[u]);l.originalType=e,l[c]="string"==typeof e?e:o,i[1]=l;for(var s=2;s<r;s++)i[s]=n[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},4381:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>s});var a=n(7462),o=(n(7294),n(3905));const r={},i="Examples",l={unversionedId:"examples",id:"examples",title:"Examples",description:"Below are some quick examples of using BuildFlow. If you are just getting started with BuildFlow we recommend starting with our walkthroughs:",source:"@site/docs/examples.md",sourceDirName:".",slug:"/examples",permalink:"/docs/examples",draft:!1,editUrl:"https://github.com/launchflow/buildflow-docs/tree/main/docs/examples.md",tags:[],version:"current",frontMatter:{},sidebar:"mainSidebar",previous:{title:"Custom Primitives",permalink:"/docs/user-guides/primitives/custom-primitives"},next:{title:"Real-Time Image Classification",permalink:"/docs/walkthroughs/realtime-image-classification"}},u={},s=[{value:"Local Examples",id:"local-examples",level:2},{value:"Local File Change Stream -&gt; DuckDB",id:"local-file-change-stream---duckdb",level:3},{value:"GCP Examples",id:"gcp-examples",level:2},{value:"GCP Pub/Sub -&gt; GCS",id:"gcp-pubsub---gcs",level:3},{value:"GCP Pub/Sub -&gt; BigQuery",id:"gcp-pubsub---bigquery",level:3},{value:"GCS File Change Stream -&gt; BigQuery",id:"gcs-file-change-stream---bigquery",level:3},{value:"AWS Examples",id:"aws-examples",level:2},{value:"AWS SQS -&gt; S3",id:"aws-sqs---s3",level:3},{value:"AWS SQS -&gt; Snowflake",id:"aws-sqs---snowflake",level:3},{value:"S3 File Change Stream -&gt; Snowflake",id:"s3-file-change-stream---snowflake",level:3}],p={toc:s},c="wrapper";function m(e){let{components:t,...n}=e;return(0,o.kt)(c,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"examples"},"Examples"),(0,o.kt)("p",null,"Below are some quick examples of using BuildFlow. If you are just getting started with BuildFlow we recommend starting with our walkthroughs:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"./walkthroughs/realtime-image-classification"},"Real-Time Image Classification"))),(0,o.kt)("p",null,"Before running any of the examples ensure you have ",(0,o.kt)("a",{parentName:"p",href:"./install"},"installed BuildFlow"),"."),(0,o.kt)("p",null,"All examples can be run with the following commands:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# Create all resources required by the pipeline\nbuildflow apply main:app\n# Run the pipeline\nbuildflow run main:app\n# Destroy resources required by the pipeline\nbuildflow destroy main:app\n")),(0,o.kt)("h2",{id:"local-examples"},"Local Examples"),(0,o.kt)("h3",{id:"local-file-change-stream---duckdb"},"Local File Change Stream -> DuckDB"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from buildflow import Flow\n\nfrom buildflow.io.local import LocalFileChangeStream\nfrom buildflow.io.duckdb import DuckDBTable\n\n# TODO(developer): Point this at the directory you would like to watch.\nDIR_TO_WATCH = TODO\n\napp = Flow()\n\n@app.pipeline(\n  source=LocalFileChangeStream(file_path=DIR_TO_WATCH)\n  sink=DuckDB(database="mydb.duckcb", table="mytable")\n)\ndef pipeline(elem):\n  return elem\n')),(0,o.kt)("h2",{id:"gcp-examples"},"GCP Examples"),(0,o.kt)("h3",{id:"gcp-pubsub---gcs"},"GCP Pub/Sub -> GCS"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import dataclasses\nimport os\nfrom datetime import datetime\nfrom typing import Any, Dict\n\nimport buildflow\nfrom buildflow.io.gcp import GCSBucket, GCPPubSubSubscription\nfrom buildflow.types.portable import FileFormat\n\n# TODO(developer): Point this at the gcp project where resources should be created.\ngcp_project = TODO\n# TODO(developer): Change this to a unique bucket name to upload your files to.\nbucket_name = TODO\n\ninput_source = GCPPubSubSubscription(\n    project_id=gcp_project,\n    subscription_name="taxi_rides",\n).options(\n    managed=True,\n    topic=GCPPubSubTopic(\n        project_id="pubsub-public-data", topic_name="taxirides-realtime"\n    ),\n)\noutput_bucket = GCSBucket(\n    file_path="taxidata.parquet",\n    file_format=FileFormat.PARQUET,\n    project_id=gcp_project,\n    bucket_name=bucket_name,\n).options(managed=True, force_destroy=True)\n\n\n@dataclasses.dataclass\nclass TaxiOutput:\n    ride_id: str\n    point_idx: int\n    latitude: float\n    longitude: float\n    timestamp: datetime\n    meter_reading: float\n    meter_increment: float\n    ride_status: str\n    passenger_count: int\n\n\napp = buildflow.Flow()\n\n@app.pipeline(source=input_source, sink=output_bucket)\ndef pipeline(element: Dict[str, Any]) -> TaxiOutput:\n    return TaxiOutput(**element)\n')),(0,o.kt)("h3",{id:"gcp-pubsub---bigquery"},"GCP Pub/Sub -> BigQuery"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import dataclasses\nimport os\nfrom datetime import datetime\nfrom typing import Any, Dict\n\nimport buildflow\nfrom buildflow.io.gcp import BigQueryTable, GCPPubSubSubscription\n\n# TODO(developer): Point this at the gcp project where resources should be created\ngcp_project = TODO\n\ninput_source = GCPPubSubSubscription(\n    project_id=gcp_project,\n    subscription_name="taxi_rides",\n).options(\n    managed=True,\n    topic=GCPPubSubTopic(\n        project_id="pubsub-public-data", topic_name="taxirides-realtime"\n    ),\n)\noutput_table = BigQueryTable(\n    project_id=gcp_project,\n    dataset_name="buildflow_output"\n    table_name="taxidata"\n).options(managed=True, destroy_protection=False)\n\n\n@dataclasses.dataclass\nclass TaxiOutput:\n    ride_id: str\n    point_idx: int\n    latitude: float\n    longitude: float\n    timestamp: datetime\n    meter_reading: float\n    meter_increment: float\n    ride_status: str\n    passenger_count: int\n\n\napp = buildflow.Flow()\n\n@app.pipeline(source=input_source, sink=output_table)\ndef pipeline(element: Dict[str, Any]) -> TaxiOutput:\n    return TaxiOutput(**element)\n')),(0,o.kt)("h3",{id:"gcs-file-change-stream---bigquery"},"GCS File Change Stream -> BigQuery"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import dataclasses\nimport datetime\nimport io\nimport json\nimport os\nfrom typing import List\n\nimport buildflow\nfrom buildflow.io.gcp import BigQueryTable, GCSBucket, GCSFileChangeStream\nfrom buildflow.types.gcp import GCSFileChangeEvent\n\n# TODO(developer): Point this at the gcp project where resources should be created.\ngcp_project = TODO\n# TODO(developer): Change this to a unique bucket name to upload your files to.\nbucket_name = TODO\n\nsource = GCSFileChangeStream(\n    gcs_bucket=GCSBucket(\n        project_id=gcp_project,\n        bucket_name=bucket_name,\n    ).options(managed=True, force_destroy=True, bucket_region="US"),\n)\nsink = BigQueryTable(\n    project_id=gcp_project,\n    dataset_name="buildflow_output",\n    table_name="buildflow_table",\n).options(managed=True, destroy_protection=False)\n\n\napp = buildflow.Flow()\n\n\n@dataclasses.dataclass\nclass Output:\n    value: int\n\n\n# Define our processor.\n@app.pipeline(source=source, sink=sink)\ndef process(gcs_file_event: GCSFileChangeEvent) -> Output:\n    json_str = gcs_file_event.blob.decode()\n    return Output(**json.loads(json_str))\n')),(0,o.kt)("h2",{id:"aws-examples"},"AWS Examples"),(0,o.kt)("h3",{id:"aws-sqs---s3"},"AWS SQS -> S3"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import dataclasses\n\nimport buildflow\nfrom buildflow.io.aws import S3Bucket, SQSQueue\nfrom buildflow.types.portable import FileFormat\n\n# TODO(developer): Change this to a unique bucket name to upload your files to.\nbucket_name = TODO\n\ninput_source = SQSQueue(\n    queue_name="input-queue"\n    aws_region="us-east-1"\n).options(managed=True)\noutput_bucket = S3Bucket(\n    file_path="output.parquet",\n    file_format=FileFormat.PARQUET,\n    aws_region="us-east-1",\n    bucket_name=bucket_name\n).options(managed=True, force_destroy=True)\n\n\napp = buildflow.Flow()\n\n\n@dataclasses.dataclass\nclass Output:\n    value: int\n\n\n@app.pipeline(source=input_source, sink=output_bucket)\ndef pipeline(element: Dict[str, Any]) -> Output:\n    return Output(**element)\n')),(0,o.kt)("h3",{id:"aws-sqs---snowflake"},"AWS SQS -> Snowflake"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import dataclasses\n\nimport buildflow\nfrom buildflow.io.aws import S3Bucket, SQSQueue\nfrom buildflow.io.snowflake import SnowflakeTable\n\n\n# TODO(developer): fill these in\nSNOWFLAKE_BUCKET_NAME = TODO\nSNOWFLAKE_ACCOUNT = TODO\nSNOWFLAKE_USER = TODO\n# NOTE: This private key file needs to be in your workscape application directory\n# to have it uploaded to LaunchFlow correctly.\n# See: https://docs.snowflake.com/en/user-guide/key-pair-auth\nSNOWFLAKE_PRIVATE_KEY_FILE = TODO\n# NOTE: These are required so Snowflake can access the S3 bucket\nAWS_ACCESS_KEY_ID = TODO\nAWS_SECRET_ACCESS_KEY = TODO\n\n\ninput_source = SQSQueue(\n    queue_name="input-queue"\n    aws_region="us-east-1"\n).options(managed=True)\noutput_table=SnowflakeTable(\n    database="buildflow-walkthrough",\n    schema="buildflow-schema",\n    table="sf-table",\n    account=SNOWFLAKE_ACCOUNT,\n    user=SNOWFLAKE_USER,\n    private_key_file=read_private_key_file(SNOWFLAKE_PRIVATE_KEY_FILE),\n    s3_bucket=S3Bucket(\n        bucket_name=SNOWFLAKE_BUCKET_NAME,\n        aws_region="us-east-1",\n    ).options(managed=True, force_destroy=True),\n).options(managed=True, database_managed=True, schema_managed=True),\n\n\napp = Flow(\n    flow_options=FlowOptions(\n        aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n    )\n)\n\n\n@dataclasses.dataclass\nclass Output:\n    value: int\n\n\n@app.pipeline(source=input_source, sink=output_table)\ndef pipeline(element: Dict[str, Any]) -> Output:\n    return Output(**element)\n')),(0,o.kt)("h3",{id:"s3-file-change-stream---snowflake"},"S3 File Change Stream -> Snowflake"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import dataclasses\n\nimport buildflow\nfrom buildflow.io.aws import S3Bucket, S3FileChangeStream\nfrom buildflow.io.snowflake import SnowflakeTable\nfrom buildflow.types.aws import S3ChangeStreamEventType, S3FileChangeEvent\nfrom buildflow.types.portable import PortableFileChangeEventType\n\n# TODO(developer): fill these in\nSNOWFLAKE_BUCKET_NAME = TODO\nSNOWFLAKE_ACCOUNT = TODO\nSNOWFLAKE_USER = TODO\n# NOTE: This private key file needs to be in your workscape application directory\n# to have it uploaded to LaunchFlow correctly.\n# See: https://docs.snowflake.com/en/user-guide/key-pair-auth\nSNOWFLAKE_PRIVATE_KEY_FILE = TODO\n# NOTE: These are required so Snowflake can access the S3 bucket\nAWS_ACCESS_KEY_ID = TODO\nAWS_SECRET_ACCESS_KEY = TODO\n\n\ninput_source=S3FileChangeStream(\n    s3_bucket=S3Bucket(\n        bucket_name=INPUT_BUCKET_NAME,\n        aws_region="us-east-1",\n    ).options(managed=True, force_destroy=True),\n)\noutput_table=SnowflakeTable(\n    database="buildflow-walkthrough",\n    schema="buildflow-schema",\n    table="sf-table",\n    account=SNOWFLAKE_ACCOUNT,\n    user=SNOWFLAKE_USER,\n    private_key_file=read_private_key_file(SNOWFLAKE_PRIVATE_KEY_FILE),\n    s3_bucket=S3Bucket(\n        bucket_name=SNOWFLAKE_BUCKET_NAME,\n        aws_region="us-east-1",\n    ).options(managed=True, force_destroy=True),\n).options(managed=True, database_managed=True, schema_managed=True),\n\n\napp = buildflow.Flow()\n\n\n@dataclasses.dataclass\nclass Output:\n    value: int\n\n\n@app.pipeline(source=input_source, sink=output_table)\ndef pipeline(s3_file_event: S3FileChangeEvent) -> Output:\n    if s3_file_event.portable_event_type != PortableFileChangeEventType.CREATED:\n        # skip non-created events\n        # S3 publishes a test notification when it is first created and we want\n        # to ensure that it doesn\'t fail.\n        return\n    return Output(**json.loads(s3_file_event.blob.decode()))\n')))}m.isMDXComponent=!0}}]);