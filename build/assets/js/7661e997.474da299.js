"use strict";(self.webpackChunkbuildflow_docs=self.webpackChunkbuildflow_docs||[]).push([[7606],{3489:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>p,default:()=>f,frontMatter:()=>u,metadata:()=>c,toc:()=>m});var o=a(7462),n=(a(7294),a(3905)),l=a(3612),i=a(4866),s=a(5162),r=a(814);const u={},p="Real-Time Image Classification",c={unversionedId:"walkthroughs/realtime-image-classification",id:"walkthroughs/realtime-image-classification",title:"Real-Time Image Classification",description:"You can follow the same walkthrough using VS Code and LaunchFlow cloud here.",source:"@site/docs/walkthroughs/realtime-image-classification.mdx",sourceDirName:"walkthroughs",slug:"/walkthroughs/realtime-image-classification",permalink:"/docs/walkthroughs/realtime-image-classification",draft:!1,editUrl:"https://github.com/launchflow/buildflow-docs/tree/main/docs/walkthroughs/realtime-image-classification.mdx",tags:[],version:"current",frontMatter:{},sidebar:"mainSidebar",previous:{title:"Examples",permalink:"/docs/examples"},next:{title:"Primitives",permalink:"/docs/category/primitives"}},d={},m=[{value:"Clone the GitHub repository",id:"clone-the-github-repository",level:3},{value:"Setup your development enviornment",id:"setup-your-development-enviornment",level:3},{value:"Code Walkthrough",id:"code-walkthrough",level:3},{value:"Repository Structure",id:"repository-structure",level:4},{value:"Set Environment Variables",id:"set-environment-variables",level:4},{value:"Code",id:"code",level:4},{value:"Create Your Resources",id:"create-your-resources",level:3},{value:"Run BuildFlow Processor",id:"run-buildflow-processor",level:3},{value:"Clean Up Your Resources",id:"clean-up-your-resources",level:3},{value:"What&#39;s Next?",id:"whats-next",level:3}],h={toc:m},k="wrapper";function f(e){let{components:t,...a}=e;return(0,n.kt)(k,(0,o.Z)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"real-time-image-classification"},"Real-Time Image Classification"),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"You can follow the same walkthrough using VS Code and LaunchFlow cloud ",(0,n.kt)("a",{parentName:"p",href:"https://docs.launchflow.com/walkthroughs/file-ingestion"},"here"),".")),(0,n.kt)("p",null,"In this walkthrough we will watch files as they are uploaded to cloud storage\nand use ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/OlafenwaMoses/ImageAI"},"ImageAI"),"\nto classify what the image contains."),(0,n.kt)("p",null,"Before completing the walkthrough ensure you have ",(0,n.kt)("a",{parentName:"p",href:"../install"},"installed BuildFlow"),"\nwith all ",(0,n.kt)("a",{parentName:"p",href:"../install#extra-dependencies"},"extra dependencies"),"."),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"All the code for this walkthrough can be found on ",(0,n.kt)("a",{href:"https://github.com/launchflow/buildflow-gcp-image-classification"},"github")),(0,n.kt)(l.Z,{type:"tip",mdxType:"Admonition"},(0,n.kt)("p",null,"In order to run this code you will need to have a GCP project created that you have access to."),(0,n.kt)("p",null,"Follow the ",(0,n.kt)("a",{parentName:"p",href:"https://developers.google.com/workspace/guides/create-project"},"Google Cloud Walkthrough")," to create a new one if you need to."))),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"All the code for this walkthrough can be found on ",(0,n.kt)("a",{href:"https://github.com/launchflow/buildflow-aws-image-classification"},"github")),(0,n.kt)(l.Z,{type:"tip",mdxType:"Admonition"},(0,n.kt)("p",null,"In order to run this code you will need to have a AWS and Snowflake account created that you have access to."),(0,n.kt)("p",null,"Follow the ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html"},"AWS Walkthrough")," to create a new one if you need to."),(0,n.kt)("p",null,"You can signup for a Snowflake trial account ",(0,n.kt)("a",{parentName:"p",href:"https://docs.snowflake.com/en/user-guide/admin-trial-account#signing-up-for-a-trial-account"},"here"),"."))),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"All the code for this walkthrough can be found on ",(0,n.kt)("a",{href:"https://github.com/launchflow/buildflow-local-image-classification"},"github")))),(0,n.kt)("p",null,"In this walkthrough we will:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Setup your development enviornment"),(0,n.kt)("li",{parentName:"ul"},"Clone the github repository"),(0,n.kt)("li",{parentName:"ul"},"Walkthrough the code"),(0,n.kt)("li",{parentName:"ul"},"Run locally")),(0,n.kt)("h3",{id:"clone-the-github-repository"},"Clone the GitHub repository"),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"bash",mdxType:"CodeBlock"},"git clone git@github.com:launchflow/buildflow-gcp-image-classification.git \ncd buildflow-gcp-image-classification\n")),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"bash",mdxType:"CodeBlock"},"git clone git@github.com:launchflow/buildflow-aws-image-classification.git \ncd buildflow-aws-image-classification\n")),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"bash",mdxType:"CodeBlock"},"git clone git@github.com:launchflow/buildflow-local-image-classification.git \ncd buildflow-local-image-classification\n"))),(0,n.kt)("h3",{id:"setup-your-development-enviornment"},"Setup your development enviornment"),(0,n.kt)("p",null,"BuildFlow supports any python version >=3.8. Ensure you have a recent\nversion of python installed with:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"python3 --version\n")),(0,n.kt)("p",null,"If you don\u2019t have a Python interpreter, you can download and install it from the\n",(0,n.kt)("a",{parentName:"p",href:"https://devguide.python.org/versions/"},"Python downloads page"),"."),(0,n.kt)("p",null,"Once you have an approriate python version installed we recommended using a python\nvirtual environment for all local development. This helps isolate the python dependencies\nfrom this project from your system installation."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n")),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"If you are on Mac and use ",(0,n.kt)("inlineCode",{parentName:"p"},"pyenv")," to manage your python installations / virtual environments\nyou will need to install ",(0,n.kt)("inlineCode",{parentName:"p"},"xz")," to ensure the model dependencies will work."),(0,n.kt)("pre",{parentName:"admonition"},(0,n.kt)("code",{parentName:"pre"},"brew install xz\npyenv uninstall <desired-python-version>\npyenv install <desired-python-version>\n"))),(0,n.kt)("h3",{id:"code-walkthrough"},"Code Walkthrough"),(0,n.kt)("p",null,"In this section we will walkthrough individual bits of the code to make sure\nyou understanding everything. You can go to the ",(0,n.kt)("a",{parentName:"p",href:"#create-your-resources"},"next section")," if you want to jump right into running."),(0,n.kt)("p",null,"Overall our BuildFlow processor will:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"listen to image uploads to a bucket"),(0,n.kt)("li",{parentName:"ul"},"run them through an image classification model"),(0,n.kt)("li",{parentName:"ul"},"finally write the output to an analysis table")),(0,n.kt)("h4",{id:"repository-structure"},"Repository Structure"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"main.py")," - this is our main python file which contains all of our logic"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"requirements.txt")," - defines all of the requirements for our code"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"mobilenet_v2-b0353104.pth")," - the image classification model we will send the images through"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},".env.template")," - template file that can be copied to ",(0,n.kt)("inlineCode",{parentName:"li"},".env")," and filled in with your values"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"image_upload.py"),' - script for "uploading" images to trigger processing'),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"founders_dog.jpg")," - example image of one of our founders dogs")),(0,n.kt)("h4",{id:"set-environment-variables"},"Set Environment Variables"),(0,n.kt)("p",null,"Before we walkthrough the code there are a couple environment variables you will need to set to ensure the pipeline points at your resources."),(0,n.kt)("p",null,"We use a ",(0,n.kt)("inlineCode",{parentName:"p"},".env")," file to store these variables. A template ",(0,n.kt)("inlineCode",{parentName:"p"},".env.template")," is provided in the repository. Copy this file to ",(0,n.kt)("inlineCode",{parentName:"p"},".env")," and fill in the values."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cp .env.template .env\n")),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"You can set ",(0,n.kt)("code",null,"GCP_PROJECT")," to the GCP project you plan to use.",(0,n.kt)("code",null,"BUCKET_NAME")," can be any unique string. BuildFlow will handle creating and resources and setting up all the wiring to ensure your pipeline can listen to file uploads to this bucket and GCP project."),(0,n.kt)(l.Z,{type:"tip",mdxType:"Admonition"},(0,n.kt)("p",null,(0,n.kt)("code",null,"GCP_SERVICE_ACCOUNT_INFO")," can be removed if you do not wish to provide custom credentials. If removed we will fall back to the ",(0,n.kt)("a",{href:"https://cloud.google.com/docs/authentication/application-default-credentials"},"default credentials")," in your environment.")),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"GCP_PROJECT_NAME = TODO\nBUCKET_NAME = TODO\n# NOTE: This only needs to be set if you would like to provide custom credentials\n# Otherwise it can be deleted.\nGCP_SERVICE_ACCOUNT_INFO = TODO\n")),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"The following environment variables need to be set:",(0,n.kt)("ul",null,(0,n.kt)("li",null,(0,n.kt)("b",null,"INPUT_BUCKET"),": the bucket files will be uploaded/downloaded from"),(0,n.kt)("li",null,(0,n.kt)("b",null,"SNOWFLAKE_BUCKET_NAME"),": the bucket our output will be staged at before writing to snowflake"),(0,n.kt)("li",null,(0,n.kt)("b",null,"SNOWFLAKE_ACCOUNT"),": the snowflake account for where to upload data to"),(0,n.kt)("li",null,(0,n.kt)("b",null,"SNOWFLAKE_USER"),": the snowflake username to authenticate as"),(0,n.kt)("li",null,(0,n.kt)("b",null,"SNOWFLAKE_PRIVATE_KEY_FILE"),": the private key file to user for authnetication with snowflake, ",(0,n.kt)("a",{href:"https://docs.snowflake.com/en/user-guide/key-pair-auth"},"follow these steps to create one.")),(0,n.kt)("li",null,(0,n.kt)("b",null,"AWS_ACCESS_KEY_ID"),": AWS access key ID. this is required for snowflake to be able to read from the bucket"),(0,n.kt)("li",null,(0,n.kt)("b",null,"AWS_SECRET_ACCESS_KEY"),": AWS secret access key for an AWS IAM user. this is required for snowflake to be able to read from the bucket"))),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"INPUT_BUCKET_NAME=TODO\nSNOWFLAKE_BUCKET_NAME=TODO\nSNOWFLAKE_ACCOUNT=TODO\nSNOWFLAKE_USER=TODO\nSNOWFLAKE_PRIVATE_KEY_FILE=TODO\nAWS_ACCESS_KEY_ID=TODO\nAWS_SECRET_ACCESS_KEY=TODO\n")),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"You can set ",(0,n.kt)("code",null,"INPUT_FOLDER_NAME")," to the folder you wish to listen to new files in. By default this points to a folder in the repository directory."),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"INPUT_FOLDER_NAME=image_folder\n"))),(0,n.kt)("h4",{id:"code"},"Code"),(0,n.kt)("p",null,"First we import all necessary modules, and ensure our environment variables from our ",(0,n.kt)("inlineCode",{parentName:"p"},".env")," file are loaded."),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"import dataclasses\nimport datetime\nimport os\nimport tempfile\nfrom typing import List\n \nimport dotenv\nfrom imageai.Classification import ImageClassification\n \nfrom buildflow import Flow, FlowOptions\nfrom buildflow.io.gcp import GCSFileChangeStream, BigQueryTable, GCSBucket\nfrom buildflow.types.gcp import GCSFileChangeEvent\n \ndotenv.load_dotenv()\n")),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"import dataclasses\nimport datetime\nimport os\nimport tempfile\nfrom typing import List\n \nimport dotenv\nfrom imageai.Classification import ImageClassification\n \nfrom buildflow import Flow, FlowOptions\nfrom buildflow.io.aws import S3FileChangeStream, S3Bucket\nfrom buildflow.io.snowflake import SnowflakeTable, read_private_key_file\nfrom buildflow.types.aws import S3FileChangeEvent, S3ChangeStreamEventType\n \ndotenv.load_dotenv()\n")),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"import dataclasses\nimport datetime\nimport os\nfrom typing import List\n \nimport dotenv\nfrom imageai.Classification import ImageClassification\n \nfrom buildflow import Flow, FlowOptions\nfrom buildflow.io.local import LocalFileChangeStream\nfrom buildflow.io.duckdb import DuckDBTable\nfrom buildflow.types.local import LocalFileChangeEvent\n \ndotenv.load_dotenv()\n"))),(0,n.kt)("p",null,"Next we define our output schema. Here we can use dataclasses to describe what the\nschema of our analysis table will be. BuildFlow will take this schema and create\na new table."),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"You can even nest dataclasses to get a nested structure in your table.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"@dataclasses.dataclass\nclass Classification:\n    classification: str\n    confidence: float\n\n\n@dataclasses.dataclass\nclass ImageClassificationRow:\n    image_name: str\n    upload: str\n    classifications: List[Classification]\n")),(0,n.kt)("p",null,"Now we create our Flow which is a container for our pipeline. We can add as many pipelines as we like, but will only use one for this walkthrough."),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Here we load in any custom service account credentials that were provided."),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},'app = Flow(\n    flow_options=FlowOptions(gcp_service_account_info=os.getenv("GCP_SERVICE_ACCOUNT_INFO")\n    )\n)\n')),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Here we load in the credentials that were provided."),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"app = Flow(\n    flow_options=FlowOptions(\n        aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n    )\n)\n")),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"app = Flow()\n"))),(0,n.kt)("p",null,"Then we define our BuildFlow pipeline, our source, and our sink. Here we are saying we want to receive file notifications for this bucket and output\nthem to our configured analysis table."),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"We use a decorter on a class because we want to be able to setup state for our model.")),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Our source is ",(0,n.kt)("a",{href:"../reference/primitives/gcp/gcs_file_change_stream"},"GCSFileChangeStream")," and our sink is a ",(0,n.kt)("a",{href:"../reference/primitives/gcp/gcp_bigquery#bigquerytable)"},"BigQueryTable"),"."),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},'@app.pipeline(\n    source=GCSFileChangeStream(\n        gcs_bucket=GCSBucket(\n            project_id=GCP_PROJECT_NAME,\n            bucket_name=BUCKET_NAME,\n        ).options(managed=True, force_destroy=True),\n    ),\n    sink=BigQueryTable(\n        project_id=GCP_PROJECT_NAME,\n        dataset_name="buildflow_walkthrough",\n        table_name="image_classification",\n    ).options(managed=True, destroy_protection=False),\n)\nclass ImageClassificationProcessor:\n')),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Our source is ",(0,n.kt)("a",{href:"../reference/primitives/aws/s3_file_change_stream"},"S3FileChangeStream")," and our sink is a ",(0,n.kt)("a",{href:"../reference/primitives/snowflake"},"SnowflakeTable"),"."),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},'@app.pipeline(\n    source=S3FileChangeStream(\n        s3_bucket=S3Bucket(\n            bucket_name=INPUT_BUCKET_NAME,\n            aws_region="us-east-1",\n        ).options(managed=True, force_destroy=True),\n    ),\n    sink=SnowflakeTable(\n        database="buildflow-walkthrough",\n        schema="buildflow-schema",\n        table="image_classification",\n        account=SNOWFLAKE_ACCOUNT,\n        user=SNOWFLAKE_USER,\n        private_key=read_private_key_file(SNOWFLAKE_PRIVATE_KEY_FILE),\n        bucket=S3Bucket(\n            bucket_name=SNOWFLAKE_BUCKET_NAME,\n            aws_region="us-east-1",\n        ).options(managed=True, force_destroy=True),\n    ).options(managed=True),\n)\n')),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Our source is ",(0,n.kt)("a",{href:"../reference/primitives/local/file_change_stream"},"LocalFileChangeStream")," and our sink is a ",(0,n.kt)("a",{href:"../reference/primitives/duckdb"},"DuckDBTable"),"."),(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},'@app.pipeline(\n    source=LocalFileChangeStream(file_path="./image_folder"),\n    sink=DuckDBTable(database="buildflow.duckdb", table="image_classification"),\n)\n'))),(0,n.kt)("p",null,"After our source and sink, we setup the state for our model. Here we use the BuildFlow\n",(0,n.kt)("a",{parentName:"p",href:"../user-guides/processors/stateful-processors"},(0,n.kt)("inlineCode",{parentName:"a"},"setup"))," method to load in our model.\nThis ensures we are only loading our model once for the lifetime of a replica."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'    def setup(self):\n        self.execution_path = os.path.dirname(os.path.realpath(__file__))\n        self.prediction = ImageClassification()\n        self.prediction.setModelTypeAsMobileNetV2()\n        self.prediction.setModelPath(\n            os.path.join(self.execution_path, "mobilenet_v2-b0353104.pth")\n        )\n        self.prediction.loadModel()\n')),(0,n.kt)("p",null,"Finally we define our process logic. This is the method that is actually called when\na new file is loaded to the bucket."),(0,n.kt)("p",null,"The logic is as follows:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Load the blob from the event "),(0,n.kt)("li",{parentName:"ol"},"Run the image through the model"),(0,n.kt)("li",{parentName:"ol"},"Convert the model output into our output schema and return it")),(0,n.kt)("p",null,"The returned object will then be written to our output table."),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},'    def process(\n        self,\n        file_event: GCSFileChangeEvent,\n    ) -> ImageClassificationRow:\n        with tempfile.TemporaryDirectory() as td:\n            file_path = os.path.join(td, file_event.file_path)\n            # Download the bytes to a local file that can be sent through the model\n            with open(file_path, "wb") as f:\n                f.write(file_event.blob)\n            predictions, probabilities = self.prediction.classifyImage(\n                file_path, result_count=5\n            )\n        classifications = []\n        for predicition, probability in zip(predictions, probabilities):\n            classifications.append(Classification(predicition, probability))\n        row = ImageClassificationRow(\n            image_name=file_event.file_path,\n            upload=datetime.datetime.utcnow().isoformat(),\n            classifications=classifications,\n        )\n        print(row)\n        return row\n')),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"    def process(\n        self,\n        file_event: S3FileChangeEvent,\n    ) -> ImageClassificationRow:\n        if file_event.event_type not in S3ChangeStreamEventType.create_event_types():\n            # s3/sqs publishes a test notification when the notification is first created\n            # we ignore that\n            return\n        with tempfile.TemporaryDirectory() as td:\n            if not os.path.exists(file_event.file_path):\n                # Check if the path exists first, if we're running locally\n                # we don't need to download and write the file.\n                file_path = os.path.join(td, file_event.file_path)\n                with open(file_path, \"wb\") as f:\n                    f.write(file_event.blob)\n            else:\n                file_path = file_event.file_path\n            predictions, probabilities = self.prediction.classifyImage(\n                file_path, result_count=5\n            )\n        classifications = []\n        for predicition, probability in zip(predictions, probabilities):\n            classifications.append(Classification(predicition, probability))\n        row = ImageClassificationRow(\n            image_name=file_event.file_path,\n            upload=datetime.datetime.utcnow().isoformat(),\n            classifications=classifications,\n        )\n        print(row)\n        return row\n")),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)(r.Z,{language:"python",mdxType:"CodeBlock"},"    def process(\n        self,\n        file_event: LocalFileChangeEvent,\n    ) -> ImageClassificationRow:\n        predictions, probabilities = self.prediction.classifyImage(\n            file_event.file_path, result_count=5\n        )\n        classifications = []\n        for predicition, probability in zip(predictions, probabilities):\n            classifications.append(Classification(predicition, probability))\n        row = ImageClassificationRow(\n            image_name=file_event.file_path,\n            upload=datetime.datetime.utcnow().isoformat(),\n            classifications=classifications,\n        )\n        print(row)\n        return row\n"))),(0,n.kt)("h3",{id:"create-your-resources"},"Create Your Resources"),(0,n.kt)("p",null,"Before you can run your processor locally you will need to create all the resources that is uses. This includes:"),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("ul",null,(0,n.kt)("li",null,"GCS bucket"),(0,n.kt)("li",null,"Pub/Sub topics and subscriptions"),(0,n.kt)("li",null,"GCS notifications"),(0,n.kt)("li",null,"BigQuery Table"))),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("ul",null,(0,n.kt)("li",null,"S3 Bucket for uploading images"),(0,n.kt)("li",null,"SQS Queue"),(0,n.kt)("li",null,"S3 Notifications"),(0,n.kt)("li",null,"SQS Queue Policy"),(0,n.kt)("li",null,"Snowflake Database"),(0,n.kt)("li",null,"Snowflake Schema"),(0,n.kt)("li",null,"Snowflake Table"),(0,n.kt)("li",null,"Snowflake Stage"),(0,n.kt)("li",null,"Snowflake Pipe"))),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},"You can skip this step for local since no resources need to be created.")),(0,n.kt)("p",null,"With BuildFlow this can all be done automtically without every having to visit the GCP cloud console."),(0,n.kt)("p",null,"To create your resources run:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"buildflow apply main:app\n")),(0,n.kt)("p",null,"This will show you what resource it is going to create and ask you to confirm. If you are happy type ",(0,n.kt)("inlineCode",{parentName:"p"},"yes"),"."),(0,n.kt)("h3",{id:"run-buildflow-processor"},"Run BuildFlow Processor"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"buildflow run main:app\n")),(0,n.kt)(i.Z,{groupId:"cloud-types",mdxType:"Tabs"},(0,n.kt)(s.Z,{value:"gcp",label:"GCP",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Once the pipeline is up and running you can run the provided script to upload images to the bucket and see output in BigQuery. Make sure to replace ",(0,n.kt)("code",null,"BUCKET_NAME")," with the name of the bucket and ",(0,n.kt)("code",null,"GCP_PROJECT")," with the GCP project used in your pipeline."),(0,n.kt)(r.Z,{language:"bash",mdxType:"CodeBlock"},"python image_upload.py --bucket=BUCKET_NAME --project=GCP_PROJECT"),(0,n.kt)(l.Z,{type:"tip",mdxType:"Admonition"},(0,n.kt)("p",null,"By default this uploads a photo of one of our contributors dogs. If you want to upload your own photo you can use the ",(0,n.kt)("code",null,"--image")," flag to specify a path to your own image."),(0,n.kt)("p",null,"If you updated the dataset or table you wrote to, you can provide the ",(0,n.kt)("code",null,"--dataset-name")," or ",(0,n.kt)("code",null,"--table-name")," flags to point the script at a different table."))),(0,n.kt)(s.Z,{value:"aws",label:"AWS",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Once the pipeline is up and running you can run the provided script to upload images to the bucket and see output in Snowflake. Make sure to replace ",(0,n.kt)("code",null,"BUCKET_NAME")," with the name of the bucket."),(0,n.kt)(r.Z,{language:"bash",mdxType:"CodeBlock"},"python image_upload.py --bucket=BUCKET_NAME"),(0,n.kt)(l.Z,{type:"tip",mdxType:"Admonition"},(0,n.kt)("p",null,"By default this uploads a photo of one of our contributors dogs. If you want to upload your own photo you can use the ",(0,n.kt)("code",null,"--image")," flag to specify a path to your own image."),(0,n.kt)("p",null,"If you updated the database, schema, or table you wrote to, you can provide ",(0,n.kt)("code",null,"--database"),", ",(0,n.kt)("code",null,"--schema"),", or ",(0,n.kt)("code",null,"--table")," flags to point the script at a different table."))),(0,n.kt)(s.Z,{value:"local",label:"Local",className:"tab-content",mdxType:"TabItem"},(0,n.kt)("p",null,"Once the pipeline is up and running you can run the provided script to copy images to the directory you are watching and have them written to your local DuckDB database."),(0,n.kt)(r.Z,{language:"bash",mdxType:"CodeBlock"},"python image_upload.py"),(0,n.kt)(l.Z,{type:"tip",mdxType:"Admonition"},(0,n.kt)("p",null,"By default this copies a photo of one of our contributors dogs. If you want to upload your own photo you can use the ",(0,n.kt)("code",null,"--image")," flag to specify a path to your own image."),(0,n.kt)("p",null,"Also if you updated the folder, duckdb database, or table you wrote to, you can provide them with ",(0,n.kt)("code",null,"--image-folder"),", ",(0,n.kt)("code",null,"--duckdb-database"),", or ",(0,n.kt)("code",null,"--duckdb-table")," flags.")))),(0,n.kt)("h3",{id:"clean-up-your-resources"},"Clean Up Your Resources"),(0,n.kt)("p",null,"Once you're all done you can clean up all your resources with a single command."),(0,n.kt)("p",null,"To clean up your resources run:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"buildflow destroy main:app\n")),(0,n.kt)("h3",{id:"whats-next"},"What's Next?"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"../key-concepts"},"Learn more about BuildFlow Concepts")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://docs.launchflow.com/vscode/overview"},"Use LaunchFlow to iterate directly in VSCode")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://docs.launchflow.com/walkthroughs/file-ingestion#launch-a-launchflow-deployment"},"Use LaunchFlow to deploy your pipeline remotely"))))}f.isMDXComponent=!0}}]);