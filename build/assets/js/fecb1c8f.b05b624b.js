"use strict";(self.webpackChunkbuildflow_docs=self.webpackChunkbuildflow_docs||[]).push([[682],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),u=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=u(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=u(n),d=a,h=c["".concat(s,".").concat(d)]||c[d]||g[d]||i;return n?r.createElement(h,o(o({ref:t},p),{},{components:n})):r.createElement(h,o({ref:t},p))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:a,o[1]=l;for(var u=2;u<i;u++)o[u]=n[u];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8609:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>g,frontMatter:()=>i,metadata:()=>l,toc:()=>u});var r=n(7462),a=(n(7294),n(3905));const i={},o="GCS CSV to GCP BigQuery Streaming",l={unversionedId:"walkthroughs/csv_bigquery_streaming",id:"walkthroughs/csv_bigquery_streaming",title:"GCS CSV to GCP BigQuery Streaming",description:"In this walkthrough we will run a BuildFlow application that listens for CSV file uploads to a Google Cloud Storage bucket. When an upload occurs the BuildFlow application will read the corresponding file, perform any necessary transformations on it, and upload the results to BigQuery. You can find all the code for this walk through here.",source:"@site/docs/walkthroughs/csv_bigquery_streaming.md",sourceDirName:"walkthroughs",slug:"/walkthroughs/csv_bigquery_streaming",permalink:"/docs/walkthroughs/csv_bigquery_streaming",draft:!1,editUrl:"https://github.com/launchflow/buildflow-docs/tree/main/docs/walkthroughs/csv_bigquery_streaming.md",tags:[],version:"current",frontMatter:{},sidebar:"mainSidebar",previous:{title:"Local GCP Pub/Sub to Parquet",permalink:"/docs/walkthroughs/local_pubsub_streaming"},next:{title:"AWS SQS Streaming",permalink:"/docs/walkthroughs/aws_sqs_streaming"}},s={},u=[{value:"Getting Started",id:"getting-started",level:2},{value:"Setting up your environment",id:"setting-up-your-environment",level:3},{value:"Run Pipeline",id:"run-pipeline",level:2},{value:"Pipeline Code",id:"pipeline-code",level:3},{value:"Cleaning Up",id:"cleaning-up",level:2}],p={toc:u},c="wrapper";function g(e){let{components:t,...i}=e;return(0,a.kt)(c,(0,r.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"gcs-csv-to-gcp-bigquery-streaming"},"GCS CSV to GCP BigQuery Streaming"),(0,a.kt)("p",null,"In this walkthrough we will run a BuildFlow application that listens for CSV file uploads to a Google Cloud Storage bucket. When an upload occurs the BuildFlow application will read the corresponding file, perform any necessary transformations on it, and upload the results to BigQuery. You can find all the code for this walk through ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/launchflow/buildflow/blob/main/buildflow/samples/csv_bigquery_walkthrough.py"},"here"),"."),(0,a.kt)("p",null,"You'll notice that with BuildFlow all you need to worry about is your transformation logic. All of the IO configuration, listening for files, and writing to BigQuery is handled by Buildflow."),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"There is a known edge case with the automatic GCS setup. This will be fixed in the next major release (coming March 31, 2023). It is recommended that you manually set up your GCS notifications for now.")),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"If you don't have a GCP project setup you can try out our ",(0,a.kt)("a",{parentName:"p",href:"/docs/walkthroughs/local_pubsub_streaming"},"local walkthrough")," which will run everything local.")),(0,a.kt)("h2",{id:"getting-started"},"Getting Started"),(0,a.kt)("p",null,"In order to follow this guide you must have a GCP project set up where Google Cloud Storage, Pub/Sub, and BigQuery can be used / created. You will also need to have the gcloud CLI installed to setup authentication / clean up resources when you are finished. Instructions for that can be found ",(0,a.kt)("a",{parentName:"p",href:"https://cloud.google.com/sdk/docs/install"},"here"),"."),(0,a.kt)("h3",{id:"setting-up-your-environment"},"Setting up your environment"),(0,a.kt)("p",null,"To interact with GCP resources BuildFlow will use the application default\ncredentials on your machine. To set those up you can run:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"gcloud auth application-default login\n")),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"If you happen to be following along from a VM running on GCP these may already\nbe setup for you.")),(0,a.kt)("p",null,"Install BuildFlow"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"pip install buildflow\n")),(0,a.kt)("h2",{id:"run-pipeline"},"Run Pipeline"),(0,a.kt)("p",null,"When running the application the following resources will be created."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Google Cloud Storage Bucket to upload files to"),(0,a.kt)("li",{parentName:"ul"},"Pub/Sub topic that will recieve notifications from the Google Cloud Storage Bucket"),(0,a.kt)("li",{parentName:"ul"},"Pub/Sub subscriber that subscribers to the Pub/Sub topic"),(0,a.kt)("li",{parentName:"ul"},"BigQuery dataset and BigQuery table where the data is written.")),(0,a.kt)("p",null,"The application does the following:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Listens for uploaded CSV files containing hourly view counts of Wikipedia pages"),(0,a.kt)("li",{parentName:"ol"},"Aggreates the view counts into daily metrics."),(0,a.kt)("li",{parentName:"ol"},"Outputs the daily aggregations to BigQuery.")),(0,a.kt)("p",null,"To run the application run:"),(0,a.kt)("admonition",{type:"note"},(0,a.kt)("p",{parentName:"admonition"},"You will need to set the GCP_PROJECT to a GCP project you can create the resources in, and BUCKET_NAME to a unique bucket name.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"python -m buildflow.samples.csv_bigquery_walkthrough --gcp_project=$GCP_PROJECT --bucket_name=$BUCKET_NAME\n")),(0,a.kt)("p",null,"Once the application is running. Download the Wiki page view CSVs ",(0,a.kt)("a",{target:"_blank",href:n(3710).Z},"here"),", and upload it to the GCS bucket using:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"gsutil cp <PATH_TO_LOCAL_CSV> gs://$BUCKET_NAME\n")),(0,a.kt)("p",null,"That will take a couple seconds to process and then you should be able to checkout the BigQuery table to view the aggregate data."),(0,a.kt)("h3",{id:"pipeline-code"},"Pipeline Code"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import argparse\nimport csv\nimport dataclasses\nimport datetime\nimport io\nimport sys\nfrom typing import List\n\nimport buildflow\nfrom buildflow import Flow\n\n# Parser to allow run time configuration of arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--gcp_project', type=str, required=True)\nparser.add_argument('--bucket_name', type=str, required=True)\nparser.add_argument('--table_name', type=str, default='csv_bigquery')\nargs, _ = parser.parse_known_args(sys.argv)\n\n# Set up a subscriber for the source.\n# The source will setup a Pub/Sub topic and subscription to listen to new files\n# uploaded to the GCS bucket.\nsource = buildflow.GCSFileNotifications(project_id=args.gcp_project,\n                                        bucket_name=args.bucket_name)\n# Set up a BigQuery table for the sink.\n# If this table does not exist yet BuildFlow will create it.\nsink = buildflow.BigQuerySink(\n    table_id=f'{args.gcp_project}.buildflow_walkthrough.{args.table_name}')\n\n\n# Nested dataclasses can be used inside of your schemas.\n@dataclasses.dataclass\nclass HourAggregate:\n    hour: datetime.datetime\n    stat: int\n\n\n# Define an output type for our application.\n# By using a dataclass we can ensure our python type hints are validated\n# against the BigQuery table's schema.\n@dataclasses.dataclass\nclass AggregateWikiPageViews:\n    date: datetime.date\n    wiki: str\n    title: str\n    daily_page_views: int\n    max_page_views_per_hour: HourAggregate\n    min_page_views_per_hour: HourAggregate\n\n\nflow = Flow()\n\n\n# Define our processor.\n@app.processor(source=source, sink=sink)\ndef process(\n        gcs_file_event: buildflow.GCSFileEvent\n) -> List[AggregateWikiPageViews]:\n    csv_string = gcs_file_event.blob.decode()\n    csv_reader = csv.DictReader(io.StringIO(csv_string))\n    aggregate_stats = {}\n    for row in csv_reader:\n        timestamp = datetime.datetime.strptime(row['datehour'],\n                                               '%Y-%m-%d %H:%M:%S.%f %Z')\n        wiki = row['wiki']\n        title = row['title']\n        views = row['views']\n\n        key = (wiki, title)\n        if key in aggregate_stats:\n            stats = aggregate_stats[key]\n            stats.daily_page_views += views\n            if views > stats.max_page_views_per_hour.stat:\n                stats.max_page_views_per_hour = HourAggregate(timestamp, views)\n            if views < stats.min_page_views_per_hour.stat:\n                stats.min_page_views_per_hour = HourAggregate(timestamp, views)\n        else:\n            aggregate_stats[key] = AggregateWikiPageViews(\n                date=timestamp.date(),\n                wiki=wiki,\n                title=title,\n                daily_page_views=views,\n                max_page_views_per_hour=HourAggregate(timestamp, views),\n                min_page_views_per_hour=HourAggregate(timestamp, views),\n            )\n\n    return list(aggregate_stats.values())\n\n\n# Run your flow.\nflow.run()()\n")),(0,a.kt)("h2",{id:"cleaning-up"},"Cleaning Up"),(0,a.kt)("p",null,"Make sure to clean up the resources you created to avoid extra GCP costs."),(0,a.kt)("p",null,"Delete the Pub/Sub subscription:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"gcloud pubsub subscriptions delete projects/$GCP_PROJECT/subscriptions/${BUCKET_NAME}_subscriber\n")),(0,a.kt)("p",null,"Delete the Pub/Sub topic:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"gcloud pubsub topics delete projects/$GCP_PROJECT/topics/${BUCKET_NAME}_notifications\n")),(0,a.kt)("p",null,"Delete the BigQuery table:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"bq rm --project_id=$GCP_PROJECT buildflow_walkthrough.csv_bigquery\n")),(0,a.kt)("p",null,"Delete the GCS Bucket:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"gcloud storage rm --recursive gs://$BUCKET_NAME/\n")))}g.isMDXComponent=!0},3710:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/files/wiki_page_views-ab8e1b599d61b471ea1e5dfaae9201f7.csv"}}]);