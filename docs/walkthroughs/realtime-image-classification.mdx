import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CloudTabs from '../../src/components/CloudTabs';
import GCPContent from '../../src/components/CloudTabs';
import AWSContent from '../../src/components/CloudTabs';
import LocalContent from '../../src/components/CloudTabs';
import CodeBlock from '@theme/CodeBlock';

# Real-Time Image Classification


In this walkthrough we will watch files as they are uploaded to cloud storage
and use [ImageAI](https://github.com/OlafenwaMoses/ImageAI)
to classify what the image contains.

Before completing the walkthrough ensure you have [installed BuildFlow](../install)
with all [extra dependencies](../install#extra-dependencies).

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<p>All the code for this walkthrough can be found at: https://github.com/launchflow/buildflow-gcp-image-classification</p>
<Admonition type="tip">

In order to run this code you will need to have a GCP project created that you have access to.

Follow the [Google Cloud Walkthrough](https://developers.google.com/workspace/guides/create-project) to create a new one if you need to.

</Admonition>
    
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

In this walkthrough we will:

- Setup your development enviornment
- Clone the github repository
- Walkthrough the code
- Run locally

### Clone the GitHub repository

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<CodeBlock language="bash">
{`git clone git@github.com:launchflow/buildflow-gcp-image-classification.git 
cd buildflow-gcp-image-classification
`}
</CodeBlock>
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

### Setup your development enviornment

BuildFlow supports any python version >=3.8. Ensure you have a recent
version of python installed with:

```
python3 --version
```

If you donâ€™t have a Python interpreter, you can download and install it from the
[Python downloads page](https://devguide.python.org/versions/).

Once you have an approriate python version installed we recommended using a python
virtual environment for all local development. This helps isolate the python dependencies
from this project from your system installation.

```
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

:::tip

If you are on Mac and use `pyenv` to manage your python installations / virtual environments
you will need to install `xz` to ensure the model dependencies will work.

```
brew install xz
pyenv uninstall <desired-python-version>
pyenv install <desired-python-version>
```

:::


### Code Walkthrough

In this section we will walkthrough individual bits of the code to make sure 
you understanding everything. You can go to the [next section](#create-your-resources) if you want to jump right into running.

Overall our BuildFlow processor will:

- listen to image uploads to a bucket
- run them through an image classification model
- finally write the output to an analysis table

#### Repository Structure

- **main.py** - this is our main python file which contains all of our logic
- **requirements.txt** - defines all of the requirements for our code
- **resnet50-19c8e357.pth** - the image classification model we will send the images through

#### Code

First we import all necessary modules.

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<CodeBlock language="python">
{`import dataclasses
import datetime
import os
import pandas as pd
import tempfile
from typing import List
 
import buildflow
from buildflow.io.gcp import GCSFileChangeStream, BigQueryTable, GCSBucket
from imageai.Classification import ImageClassification
`}
</CodeBlock>

<p>Then we define some constants for which GCP project to use and what the name of
the bucket should be. You can set `GCP_PROJECT` to the GCP project you plan to use.
'BUCKET_NAME' can be any unique string. BuildFlow will handle creating and resources and 
setting up all the wiring to ensure your pipeline can listen to file uploads to this bucket.</p>


<CodeBlock language="python">
{`# TODO(developer): fill these in.
GCP_PROJECT = TODO
BUCKET_NAME = TODO
`}
</CodeBlock>
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

Next we define our output schema. Here we can use dataclasses to describe what the
schema of our analysis table will be. BuildFlow will take this schema and create 
a new table.

:::tip

You can even nest dataclasses to get a nested structure in your BigQuery table.

:::

```python
@dataclasses.dataclass
class Classification:
    classification: str
    confidence: float


@dataclasses.dataclass
class ImageClassificationRow:
    image_name: str
    upload: str
    classifications: List[Classification]
```

Now we create our Flow which is a container for our pipeline. We can add as many pipelines as we like, but will only use one for this walkthrough.

```python
app = Flow()
```

Then we define our BuildFlow pipeline, our source, and our sink. Here we are saying we want to receive file notifications for this bucket and output
them to our configured analysis table.
:::tip

We use our decorter on a class because we want to be able to setup state for our model.

:::

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<p>
Our source is <a href="../reference/primitives/gcp/gcs_file_change_stream">GCSFileChangeStream</a> and our sink is a <a href="../reference/primitives/gcp/gcp_bigquery#bigquerytable)">BigQueryTable</a>.
</p>
<CodeBlock language="python">
{`@app.pipeline(
    source=GCSFileChangeStream(
        gcs_bucket=GCSBucket(
            project_id=GCP_PROJECT_NAME,
            bucket_name=BUCKET_NAME,
            bucket_region="us-central1",
        ).options(managed=True, force_destroy=True),
    ),
    sink=BigQueryTable(
        project_id=GCP_PROJECT_NAME,
        dataset_name="buildflow_walkthrough",
        table_name="image_classification",
    ).options(managed=True, destroy_protection=False),
)
class ImageClassificationProcessor:
`}
</CodeBlock>
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

After our source and sink, we setup the state for our model. Here we use the BuildFlow
[`setup`](../user-guides/pipelines/stateful-pipelines) method to load in our model.
This ensures we are only loading our model once for the lifetime of a replica.

```python
    def setup(self):
        self.execution_path = os.path.dirname(os.path.realpath(__file__))
        self.prediction = ImageClassification()
        self.prediction.setModelTypeAsResNet50()
        self.prediction.setModelPath(
            os.path.join(self.execution_path, "resnet50-19c8e357.pth"))
        self.prediction.loadModel()
```

Finally we define our process logic. This is the method that is actually called when
a new file is loaded to the bucket.

The logic is as follows:

1. Load the blob from the event 
2. Run the image through the model
3. Convert the model output into our output schema and return it

The returned object will then be written to our output table.

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<CodeBlock language="python">
{`    def process(
        self, gcs_file_event: buildflow.io.GCSFileEvent
    ) -> ImageClassificationRow:
        with tempfile.TemporaryDirectory() as td:
            file_path = os.path.join(td, gcs_file_event.metadata['objectId'])
            with open(file_path,'wb') as f:
                f.write(gcs_file_event.blob)
            predictions, probabilities = self.prediction.classifyImage(
                file_path, result_count=5)
        classifications = []
        for predicition, probability in zip(predictions, probabilities):
            classifications.append(Classification(predicition, probability))
        return ImageClassificationRow(
            image_name=gcs_file_event.metadata['objectId'],
            upload=pd.Timestamp(gcs_file_event.metadata['eventTime']),
            classifications=classifications,
        )
`}
</CodeBlock>
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

### Create Your Resources

Before you can run your processor locally you will need to create all the resources that is uses. This includes:

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<ul>
    <li>GCS bucket</li>
    <li>Pub/Sub topics and subscriptions</li>
    <li>GCS notifications</li>
    <li>BigQuery Table</li>
</ul>
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

With BuildFlow this can all be done automtically without every having to visit the GCP cloud console.

To create your resources run:

```
buildflow apply main:app
```

This will show you what resource it is going to create and ask you to confirm. If you are happy type `yes`.

### Run BuildFlow Processor

```
buildflow run main:app
```

<Tabs groupId="cloud-types">
    <TabItem value="gcp" label="GCP" className="tab-content">
<p>Once the pipeline is up and running you can run the provided script to upload images to the bucket and see output in BigQuery.
Make sure to replace <code>BUCKET_NAME</code> with the name of the bucket and <code>GCP_PROJECT</code> with the GCP project used in your pipeline.</p>

<CodeBlock language="bash">
python image_upload.py --bucket=BUCKET_NAME --project=GCP_PROJECT
</CodeBlock>
<Admonition type="tip">
<p>By default this uploads a photo of one of our contributors dogs. If you want to upload your own photo you can use the <code>--image</code> flag to specify a path to your own image.</p>

<p>Also if you updated the dataset or table you wrote to you can provide <code>--dataset-name</code> and <code>--table-name</code> to point the script at a different table.</p>

</Admonition>
    </TabItem>
    <TabItem value="aws" label="AWS" className="tab-content">
Coming soon!
    </TabItem>
    <TabItem value="local" label="Local" className="tab-content">
Coming soon!
    </TabItem>
</Tabs>

### Clean Up Your Resources

Once you're all done you can clean up all your resources with a single command.

To clean up your resources run:

```
buildflow destroy main:app
```

### What's Next?

- [Learn more about BuildFlow Concepts](../key-concepts)
- [Use LaunchFlow to iterate directly in VSCode](https://docs.launchflow.com/vscode/overview)
- [Use LaunchFlow to deploy your pipeline remotely](https://docs.launchflow.com/walkthroughs/file-ingestion#launch-a-launchflow-deployment)